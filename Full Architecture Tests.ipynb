{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";  # The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";  # Do other imports now...\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dot, Add, Flatten, Concatenate, Embedding, LSTM, GRU, Input, Dense, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, mean_squared_error\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed_value = 42\n",
    "def init_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    \n",
    "init_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Family-Intervals Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(name):\n",
    "    with open(name, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "family_intervals = read_dict('../Family_Intervals/intervals.json')\n",
    "\n",
    "for key in family_intervals.keys():\n",
    "    if key != 'other':\n",
    "        lower, upper = family_intervals[key]\n",
    "        family_intervals[key] = range(lower, upper + 1)\n",
    "\n",
    "index_to_name = read_dict('../Family_Intervals/Family_Indexes.json')\n",
    "index_to_interval = []\n",
    "for key in index_to_name.keys():\n",
    "    index_to_interval += [family_intervals[index_to_name[key]]]\n",
    "index_to_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Top n individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mask(family,indlength=480):\n",
    "    family_intervals = index_to_interval\n",
    "    mask = np.array([0] * indlength)\n",
    "\n",
    "    for family_mask_index in range(len(family)):\n",
    "        family_interval = family_intervals[family_mask_index]\n",
    "        for feature_index in family_interval:\n",
    "            mask[feature_index] = family[family_mask_index]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# returns all unique inds that were present in an expr\n",
    "def get_all_inds(pop_dict, with_family=False):\n",
    "    all_inds = set()\n",
    "    gens = list(pop_dict.keys())\n",
    "    for gen_number in gens:\n",
    "        pop_gen = pop_dict[gen_number]\n",
    "        if with_family:\n",
    "            pop_gen = [tuple([tuple(i[0]), tuple(i[1])]) for i in pop_gen]\n",
    "        else:\n",
    "            pop_gen = [tuple(i) for i in pop_gen]\n",
    "        all_inds.update(pop_gen)\n",
    "    \n",
    "    return all_inds\n",
    "\n",
    "def get_top_n_from_gens(pop_dict, fitness_dict, n, top_biggest = False, gen=None, with_family=False):\n",
    "    inds = None\n",
    "    \n",
    "    if gen is None:\n",
    "        inds = get_all_inds(pop_dict, with_family)\n",
    "    else:\n",
    "        inds = list(map(lambda i:tuple(i), pop_dict[gen]))\n",
    "        \n",
    "    inds = list(inds)\n",
    "    inds.sort(key =  lambda ind: fitness_dict[str(ind)] if str(ind) in fitness_dict else 0)\n",
    "    \n",
    "    if top_biggest:\n",
    "        inds.reverse()\n",
    "    \n",
    "    desired_inds = inds[0:n]\n",
    "    inds_metric = list(map(lambda ind : fitness_dict[str(ind)] if str(ind) in fitness_dict else 0, desired_inds))\n",
    "    \n",
    "    \n",
    "    if with_family:\n",
    "        desired_inds = [np.multiply(np.array(i[0]), np.array(get_mask(i[1]))) for i in desired_inds]\n",
    "        \n",
    "    return desired_inds, inds_metric\n",
    "\n",
    "def get_top_n_ind_from_expr(folder, n, top_biggest = False, gen=None, with_family=False):\n",
    "    fitness_dict = read_dict(folder + \"fitness_dict.json\")\n",
    "    pop_dict = read_dict (folder + \"gens_dict.json\")\n",
    "    \n",
    "    key_func = lambda ind: fitness_dict[str(ind)]\n",
    "    return get_top_n_from_gens(pop_dict, fitness_dict, n, top_biggest = top_biggest, gen=gen, with_family=with_family)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top individual from the only-pred experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_inds_only_predict,top_metric = get_top_n_ind_from_expr(\n",
    "    folder = '../Results/GA_RES_17_4_TOURNAMNET_ONLY_PRED/' ,n = 1 ,top_biggest = True, with_family=False)\n",
    "\n",
    "best_only_predict = top_inds_only_predict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top individual from the full-train experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_inds_full_train,top_metric2 = get_top_n_ind_from_expr(\n",
    "    folder = '../Results/GA_RES_17_4_TOURNAMNET_FULL_TRAIN/' ,n = 1,top_biggest = True, with_family=False)\n",
    "best_full_train = top_inds_full_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Contextual Information For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_column = \"userId\"\n",
    "target_column = \"triggerValue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_test_val_split(arr):\n",
    "    # train = 70%\n",
    "    # test = 20% \n",
    "    # val = 10%\n",
    "    x_train, x_test = train_test_split(arr, test_size=0.3 , shuffle=False)\n",
    "    x_val ,x_test = train_test_split(x_test, test_size=2/3 , shuffle = False)\n",
    "    return x_train, x_test, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, y_val = np.load('../data/y_train.npy'), np.load('../data/y_test.npy'),np.load('../data/y_val.npy')\n",
    "item_train, item_test, item_val = np.load('../data/item_train.npy'),np.load('../data/item_test.npy'),np.load('../data/item_val.npy')\n",
    "user_train, user_test, user_val = np.load('../data/user_train.npy'),np.load('../data/user_test.npy'),np.load('../data/user_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_features(ind, arr):\n",
    "    res = []\n",
    "    for i in range(len(ind)):\n",
    "        if ind[i] == 1:\n",
    "            res += [arr[i]]\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def reduce_by_ind(x_arr, ind):\n",
    "    return np.array(list(map(lambda row : reduce_features(ind, row) , x_arr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_train, context_test, context_val = np.load('../data/context_train.npy'),np.load('../data/context_test.npy'),np.load('../data/context_val.npy')\n",
    "pca_context_train, pca_context_test, pca_context_val = np.load('../data/context_train_pca.npy'),np.load('../data/context_test_pca.npy'),np.load('../data/context_val_pca.npy')\n",
    "autoenc_context_train, autoenc_context_test, autoenc_context_val = np.load('../data/context_train_autoenc.npy'),np.load('../data/context_test_autoenc.npy'),np.load('../data/context_val_autoenc.npy')\n",
    "\n",
    "assert len(context_test) == len(pca_context_test) and len(context_test) == len(autoenc_context_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes for CAMF Feature (weather etc..)\n",
    "baseline_indexes = [6, 7, 8, 9, 10, 11]\n",
    "\n",
    "def reduce_by_indexes(arr, indexes):\n",
    "    res = []\n",
    "    for ind in indexes:\n",
    "        res += [arr[ind]]\n",
    "    return np.array(res)\n",
    "\n",
    "def map_reduce_indexes(arr, indexes):\n",
    "    return np.array(list(map(lambda element: reduce_by_indexes(element, indexes), arr)))\n",
    "\n",
    "baseline_train = map_reduce_indexes(context_train, baseline_indexes)\n",
    "baseline_test = map_reduce_indexes(context_test, baseline_indexes)\n",
    "baseline_val  = map_reduce_indexes(context_val, baseline_indexes)\n",
    "\n",
    "assert len(baseline_train[0]) == len(baseline_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_indexes = np.load('../feature_selections/CARS_RESULTS/feed_forward/feed_forward.npy')\n",
    "\n",
    "feed_forward_train = map_reduce_indexes(context_train, feed_forward_indexes)\n",
    "feed_forward_test = map_reduce_indexes(context_test, feed_forward_indexes)\n",
    "feed_forward_val  = map_reduce_indexes(context_val, feed_forward_indexes)\n",
    "\n",
    "assert len(feed_forward_indexes) == len(feed_forward_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BE_indexes = np.load('../feature_selections/CARS_RESULTS/backwards_elimination.npy')\n",
    "\n",
    "BE_train = map_reduce_indexes(context_train, BE_indexes)\n",
    "BE_test = map_reduce_indexes(context_test, BE_indexes)\n",
    "BE_val  = map_reduce_indexes(context_val, BE_indexes)\n",
    "\n",
    "assert len(BE_indexes) == len(BE_train[0])\n",
    "len(BE_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlypred_context_train = reduce_by_ind(context_train, best_only_predict)\n",
    "onlypred_context_test = reduce_by_ind(context_test, best_only_predict)\n",
    "onlypred_context_val = reduce_by_ind(context_val, best_only_predict)\n",
    "\n",
    "assert len(onlypred_context_train) == len(context_train) and len(onlypred_context_val) == len(context_val) and len(onlypred_context_test) == len(context_test)\n",
    "assert len(onlypred_context_train[0]) == sum(best_only_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltrain_context_train = reduce_by_ind(context_train, best_full_train)\n",
    "fulltrain_context_test = reduce_by_ind(context_test, best_full_train)\n",
    "fulltrain_context_val = reduce_by_ind(context_val, best_full_train)\n",
    "\n",
    "assert len(fulltrain_context_train) == len(context_train) and len(fulltrain_context_val) == len(context_val) and len(fulltrain_context_test) == len(context_test)\n",
    "assert len(fulltrain_context_train[0]) == sum(best_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder_3_train = np.load('../data/LSTM/LSTM_AUTO_ENCODER_3_x_train.npy')\n",
    "lstm_autoencoder_3_test = np.load('../data/LSTM/LSTM_AUTO_ENCODER_3_x_test.npy')\n",
    "lstm_autoencoder_3_val = np.load('../data/LSTM/LSTM_AUTO_ENCODER_3_x_val.npy')\n",
    "\n",
    "assert len(context_train) == len(lstm_autoencoder_3_train) and len(context_val) == len(lstm_autoencoder_3_val) and len(context_test) == len(lstm_autoencoder_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder_4_train = np.load('../data/LSTM/LSTM_AUTO_ENCODER_4_x_train.npy')\n",
    "lstm_autoencoder_4_test = np.load('../data/LSTM/LSTM_AUTO_ENCODER_4_x_test.npy')\n",
    "lstm_autoencoder_4_val = np.load('../data/LSTM/LSTM_AUTO_ENCODER_4_x_val.npy')\n",
    "\n",
    "assert len(context_train) == len(lstm_autoencoder_4_train) and len(context_val) == len(lstm_autoencoder_4_val) and len(context_test) == len(lstm_autoencoder_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder_5_train = np.load('../data/LSTM/LSTM_AUTO_ENCODER_5_x_train.npy')\n",
    "lstm_autoencoder_5_test = np.load('../data/LSTM/LSTM_AUTO_ENCODER_5_x_test.npy')\n",
    "lstm_autoencoder_5_val = np.load('../data/LSTM/LSTM_AUTO_ENCODER_5_x_val.npy')\n",
    "\n",
    "assert len(context_train) == len(lstm_autoencoder_5_train) and len(context_val) == len(lstm_autoencoder_5_val) and len(context_test) == len(lstm_autoencoder_5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_dimensions = len(context_train[0])\n",
    "context_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_unique = max([max(user_test), max(user_train), max(user_val)]) + 1\n",
    "print(source_unique)\n",
    "\n",
    "target_unique = max([max(item_test), max(item_train), max(item_val)]) + 1\n",
    "print(target_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting-up the contextual modeling architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "default_info = {'final_activation':'sigmoid', 'loss':'binary_crossentropy',\n",
    "                     'dropout' : True , 'batch_norm' : True\n",
    "                   }\n",
    "\n",
    "def get_full_arch(context_dim, info=None):\n",
    "    global default_info\n",
    "    \n",
    "    if info is None:\n",
    "        info = default_info\n",
    "    \n",
    "    loss = info['loss']\n",
    "    final_activation = info['final_activation']\n",
    "    BATCH_NORM_MLP = info['batch_norm']\n",
    "    DROPOUT_ENS_LAYER = info['dropout']\n",
    "    \n",
    "    \n",
    "    embedding_dim = 16\n",
    "    print(\"FULL ARCH {}, emb_dim = {}, loss = {}, activation = {}\".format(context_dim, embedding_dim, loss, final_activation))\n",
    "    input_target = Input(shape=[1], name=\"input_item\")\n",
    "    input_source = Input(shape=[1], name=\"input_user\")\n",
    "    input_context = Input(shape=[context_dim], name=\"input_context\")\n",
    "\n",
    "    target_emb = Embedding(input_dim=target_unique, output_dim=embedding_dim,\n",
    "                           input_length=1, name='target_emb')(input_target)\n",
    "    source_emb = Embedding(input_dim=source_unique, output_dim=embedding_dim,\n",
    "                           input_length=1, name='source_emb')(input_source)\n",
    "\n",
    "    target_emb_mlp = Embedding(input_dim=target_unique, output_dim=embedding_dim,\n",
    "                              input_length=1, name='target_emb_mlp')(input_target)\n",
    "    source_emb_mlp = Embedding(input_dim=source_unique, output_dim=embedding_dim,input_length=1, name='source_emb_mlp')(input_source)\n",
    "\n",
    "    # adding bias\n",
    "    target_bias = Embedding(input_dim=target_unique, output_dim=1,\n",
    "                            input_length=1, name='target_bias')(input_target)\n",
    "    source_bias = Embedding(input_dim=source_unique, output_dim=1,\n",
    "                            input_length=1, name='source_bias')(input_source)\n",
    "\n",
    "    target_flat = Flatten()(target_emb)\n",
    "    source_flat = Flatten()(source_emb)\n",
    "\n",
    "    target_flat_mlp = Flatten()(target_emb_mlp)\n",
    "    source_flat_mlp = Flatten()(source_emb_mlp)\n",
    "\n",
    "    concat = Concatenate()([target_flat_mlp, source_flat_mlp, input_context])\n",
    "\n",
    "    target_flat_bias = Flatten()(target_bias)\n",
    "    source_flat_bias = Flatten()(source_bias)\n",
    "    \n",
    "    mlp_dense_input = Dense(200, name='mlp_dense_in', activation='relu')(concat)\n",
    "    b0 = BatchNormalization()(mlp_dense_input)\n",
    "    mlp_hidden_0 = Dense(100, name='mlp_hidden_0', activation='relu')(b0)\n",
    "\n",
    "    b1 = BatchNormalization()(mlp_hidden_0)\n",
    "    mlp_hidden_1 = Dense(50, name='mlp_hidden_1', activation='relu')(b1)\n",
    "    b2 = BatchNormalization()(mlp_hidden_1)    \n",
    "    mlp_dense_out = Dense(1, name='mlp_dense_out', activation='sigmoid')(b2)\n",
    "         \n",
    "\n",
    "    dot = Dot(axes=1, name='dot')([source_flat, target_flat])\n",
    "\n",
    "    add = Add()([dot, target_flat_bias, source_flat_bias])\n",
    "\n",
    "    # creating a 1*2 vector that is a concatenation between the two sides\n",
    "    concat_left_and_right = Concatenate()([add, mlp_dense_out])\n",
    "    \n",
    "    # I used this for the best results so far\n",
    "    concat_left_and_right = Dropout(rate = 0.5, seed = 42)(concat_left_and_right)\n",
    "    \n",
    "    last_layers_neurons = 6\n",
    "    print(f\"last layer neurons = {last_layers_neurons}\")\n",
    "    final_mlp_dense_input = Dense(last_layers_neurons, name='final_dense_in', activation = 'relu')(concat_left_and_right)\n",
    "    final_mlp_dense_out =  Dense(1, name='final_dense_out', activation=final_activation)(final_mlp_dense_input)\n",
    "\n",
    "    # output = Activation('sigmoid')(add)\n",
    "\n",
    "    model = Model(inputs=[input_target, input_source, input_context], outputs=final_mlp_dense_out)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['mae'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_arch_no_context():\n",
    "    embedding_dim = 16\n",
    "    print(\"FULL ARCH NO CONTEXT _ emb dim = {}\".format(embedding_dim))\n",
    "\n",
    "    \n",
    "    input_target = Input(shape=[1], name=\"input_item\")\n",
    "    input_source = Input(shape=[1], name=\"input_user\")\n",
    "\n",
    "    target_emb = Embedding(input_dim=target_unique, output_dim=embedding_dim,\n",
    "                           input_length=1, name='target_emb')(input_target)\n",
    "    source_emb = Embedding(input_dim=source_unique, output_dim=embedding_dim,\n",
    "                           input_length=1, name='source_emb')(input_source)\n",
    "\n",
    "    target_emb_mlp = Embedding(input_dim=target_unique, output_dim=embedding_dim,\n",
    "                              input_length=1, name='target_emb_mlp')(input_target)\n",
    "    source_emb_mlp = Embedding(input_dim=source_unique, output_dim=embedding_dim,input_length=1, name='source_emb_mlp')(input_source)\n",
    "\n",
    "    # adding bias\n",
    "    target_bias = Embedding(input_dim=target_unique, output_dim=1,\n",
    "                            input_length=1, name='target_bias')(input_target)\n",
    "    source_bias = Embedding(input_dim=source_unique, output_dim=1,\n",
    "                            input_length=1, name='source_bias')(input_source)\n",
    "\n",
    "    target_flat = Flatten()(target_emb)\n",
    "    source_flat = Flatten()(source_emb)\n",
    "\n",
    "    target_flat_mlp = Flatten()(target_emb_mlp)\n",
    "    source_flat_mlp = Flatten()(source_emb_mlp)\n",
    "\n",
    "    concat = Concatenate()([target_flat_mlp, source_flat_mlp])\n",
    "\n",
    "    target_flat_bias = Flatten()(target_bias)\n",
    "    source_flat_bias = Flatten()(source_bias)\n",
    "    \n",
    "    mlp_dense_input = Dense(100, name='mlp_dense_in', activation='relu')(concat)\n",
    "    mlp_hidden_1 = Dense(50, name='mlp_hidden_1', activation='relu')(mlp_dense_input)\n",
    "    mlp_dense_out = Dense(1, name='mlp_dense_out', activation='sigmoid')(mlp_hidden_1)\n",
    "         \n",
    "\n",
    "    dot = Dot(axes=1, name='dot')([source_flat, target_flat])\n",
    "\n",
    "    add = Add()([dot, target_flat_bias, source_flat_bias])\n",
    "\n",
    "    # creating a 1*2 vector that is a concatenation between the two sides\n",
    "    concat_left_and_right = Concatenate()([add, mlp_dense_out])\n",
    "    \n",
    "    final_mlp_dense_input = Dense(3, name='final_dense_in', activation = 'relu')(concat_left_and_right)\n",
    "    final_mlp_dense_out =  Dense(1, name='final_dense_out', activation='sigmoid')(final_mlp_dense_input)\n",
    "\n",
    "    model = Model(inputs=[input_target, input_source], outputs=final_mlp_dense_out)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metric_dict():\n",
    "    metric_dict = {}\n",
    "    metric_dict[\"auc_vals\"] = []\n",
    "    metric_dict[\"auc_tests\"] = []\n",
    "    metric_dict[\"mse_vals\"] = []\n",
    "    metric_dict[\"mse_tests\"] = []\n",
    "    metric_dict[\"logloss_vals\"] = []\n",
    "    metric_dict[\"logloss_tests\"] = []\n",
    "    metric_dict[\"y_val_pred\"] = []\n",
    "    metric_dict[\"y_test_pred\"] = []\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "def evaluate_model(model, fresh_weights_path, x_train, x_val, x_test, test_name):\n",
    "    print(\"evaluating\")\n",
    "    model.load_weights(fresh_weights_path)\n",
    "    \n",
    "    best_arch_path = '../data/best_arch_{}.h5'.format(test_name)\n",
    "    best_adam_weights = '../data/best_adam_weights_{}.h5'.format(test_name)\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')\n",
    "    mcp = ModelCheckpoint(best_arch_path , save_best_only = True, mode = 'min')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae'])\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
    "                        epochs=100, batch_size=256, verbose=verbose, callbacks=[es,mcp])\n",
    "    \n",
    "    model = load_model(best_arch_path)\n",
    "    model.save_weights(best_adam_weights)\n",
    "    \n",
    "    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['mae'])\n",
    "    model.load_weights(best_adam_weights)\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
    "                        epochs=100, batch_size=256, verbose=verbose, callbacks=[es,mcp])\n",
    "    \n",
    "    model = load_model(best_arch_path)\n",
    "    \n",
    "    \n",
    "    val_predict = model.predict(x_val, batch_size=256)\n",
    "    \n",
    "    auc_val = roc_auc_score(y_val, val_predict)\n",
    "    mse_val = mean_squared_error(y_val, val_predict)\n",
    "    logloss_val = log_loss(y_val , val_predict)\n",
    "    \n",
    "    \n",
    "    test_predict = model.predict(x_test, batch_size=256)\n",
    "    \n",
    "    auc_test = roc_auc_score(y_test, test_predict)\n",
    "    mse_test = mean_squared_error(y_test, test_predict)\n",
    "    logloss_test = log_loss(y_test, test_predict)\n",
    "    \n",
    "    metric_dict = init_metric_dict()\n",
    "    \n",
    "    del es\n",
    "    del mcp\n",
    "    \n",
    "    metric_dict[\"auc_vals\"] = auc_val\n",
    "    metric_dict[\"auc_tests\"] = auc_test\n",
    "    metric_dict[\"mse_vals\"] = mse_val\n",
    "    metric_dict[\"mse_tests\"] = mse_test\n",
    "    metric_dict[\"logloss_vals\"] = logloss_val\n",
    "    metric_dict[\"logloss_tests\"] = logloss_test\n",
    "    metric_dict[\"y_val_pred\"] = val_predict\n",
    "    metric_dict[\"y_test_pred\"] = test_predict\n",
    "        \n",
    "    return metric_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(n_samples, eval_func):\n",
    "    metric_dict = init_metric_dict()\n",
    "    \n",
    "    for i in range(n_samples):    \n",
    "        metric_dict_current = eval_func()\n",
    "        \n",
    "        for key in metric_dict.keys():\n",
    "            metric_dict[key] += [metric_dict_current[key]]\n",
    "            \n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch_testing_wrapper(args):\n",
    "    return start_arch_testing(*args)\n",
    "\n",
    "def start_arch_testing(con_train, con_test, con_val, samples=5, model_type=\"FULL\", info = None,name_=\"\"):\n",
    "    init_seeds()\n",
    "    dim = len(con_train[0])\n",
    "    weights_path = '../data/BEST_WEIGHTS_{}'.format(name_)\n",
    "    global default_info\n",
    "    \n",
    "    if info is None:\n",
    "        info = default_info\n",
    "    else:\n",
    "        for key in default_info.keys():\n",
    "            if key not in info:\n",
    "                info[key] = default_info[key]\n",
    "\n",
    "    x_train = [item_train, user_train]\n",
    "    x_val = [item_val, user_val]\n",
    "    x_test = [item_test, user_test]\n",
    "    \n",
    "    if model_type == \"FULL\":\n",
    "        model = get_full_arch(dim, info)\n",
    "        x_train += [con_train]\n",
    "        x_val += [con_val]\n",
    "        x_test += [con_test]\n",
    "        \n",
    "    elif model_type == \"MLP\":\n",
    "        model = get_simple_mlp(dim)\n",
    "        x_train = con_train\n",
    "        x_val = con_val\n",
    "        x_test = con_test\n",
    "    \n",
    "    elif model_type == \"RIGHT\":\n",
    "        model = get_right_side(dim)\n",
    "        x_train += [con_train]\n",
    "        x_val += [con_val]\n",
    "        x_test += [con_test] \n",
    "        \n",
    "    elif model_type == \"LEFT\":\n",
    "        model = get_left_side()\n",
    "    \n",
    "    elif model_type == 'FULL_NO_CONTEXT':\n",
    "        model = get_full_arch_no_context()\n",
    "    \n",
    "    elif model_type == 'EXTENDED':\n",
    "        dims = [len(x[0]) for x in con_train]\n",
    "        model = get_full_arch_extended(dims)\n",
    "        x_train += con_train\n",
    "        x_test += con_test\n",
    "        x_val += con_val\n",
    "        print([len(z) for z in x_train])\n",
    "        \n",
    "\n",
    "    model.save_weights(weights_path)\n",
    "    return sample_model(samples, lambda : evaluate_model(model, weights_path, x_train = x_train, x_val = x_val, x_test = x_test, test_name=name_))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = 1\n",
    "m_type = \"FULL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vanilla = 'All-Context-No-Reduction'\n",
    "name_auto = 'Latent-AE'\n",
    "name_pca = 'Latent-PCA'\n",
    "name_full_train = 'BestInd-Full-Train'\n",
    "name_only_pred = 'OnlyPred'\n",
    "name_user_item = 'NeuMF'\n",
    "name_baseline = \"Explicit (CAMF)\"\n",
    "name_extended = \"EXTENDED NeuMF\"\n",
    "name_feed_forward = \"FF Feature selection\"\n",
    "name_BE = \"BE Feature Selection\"\n",
    "name_LSTM_3 = \"LSTM Autoencoder (3)\"\n",
    "name_LSTM_4 = \"LSTM Autoencoder (4)\"\n",
    "name_LSTM_5 = \"LSTM Autoencoder (5)\"\n",
    "\n",
    "\n",
    "torun=[\n",
    "    (context_train, context_test, context_val,samp, m_type, None, name_vanilla)\n",
    "   ,(pca_context_train, pca_context_test, pca_context_val, samp, m_type, None ,name_pca)\n",
    "   ,(autoenc_context_train, autoenc_context_test, autoenc_context_val,samp, m_type, None, name_auto)\n",
    "   ,(onlypred_context_train, onlypred_context_test, onlypred_context_val,samp, m_type, None, name_only_pred)\n",
    "   ,(fulltrain_context_train, fulltrain_context_test, fulltrain_context_val,samp, m_type, None, name_full_train)\n",
    "   ,(context_train, context_test, context_val,samp, \"FULL_NO_CONTEXT\", None, name_user_item)\n",
    "   ,(baseline_train, baseline_test, baseline_val ,samp, m_type,  {'dropout' : False , 'batch_norm' : False}, name_baseline)\n",
    "    \n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "result_dicts = []\n",
    "\n",
    "for run in tqdm(torun):\n",
    "    result_dicts += [arch_testing_wrapper(run)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "order = [name_user_item, name_pca, name_auto, name_full_train, name_only_pred, name_vanilla, name_baseline]\n",
    "\n",
    "def load_order_results():\n",
    "    global order\n",
    "    res = []\n",
    "    for name in order:\n",
    "        try:\n",
    "            with open('{}{}.p'.format('../FULL_ARCH_RESULTS/CARS_DS/monday/', name), 'rb') as fp:\n",
    "                res += [pickle.load(fp)]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    return res\n",
    "\n",
    "path_to_save = '../FULL_ARCH_RESULTS/CARS_DS/paper/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_results = []\n",
    "for name in order:\n",
    "    for i, run in enumerate(torun):\n",
    "        r_name = run[-1]\n",
    "        if r_name == name:\n",
    "            order_results += [result_dicts[i]]\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(name):\n",
    "    global order_results, order, torun\n",
    "    \n",
    "    update_data = None\n",
    "    \n",
    "    for run in torun:\n",
    "        if run[-1] == name:\n",
    "            update_data = run\n",
    "            break\n",
    "    \n",
    "    assert update_data is not None\n",
    "    \n",
    "    order_results[order.index(name)] = arch_testing_wrapper(update_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "try :\n",
    "    path_to_save = stamp\n",
    "    path_to_save = '../FULL_ARCH_RESULTS/CARS_DS/{}/'.format(stamp)\n",
    "    print(\"Exists\")\n",
    "except NameError:\n",
    "    print(\"New Stamp!\")\n",
    "    stamp = datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")    \n",
    "    path_to_save = '../FULL_ARCH_RESULTS/CARS_DS/{}/'.format(stamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_current_model_and_results():\n",
    "    try:\n",
    "        os.mkdir(path_to_save)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # python 3.x\n",
    "        import pickle\n",
    "\n",
    "    for i, name in enumerate(order):\n",
    "        data = order_results[i]    \n",
    "        with open('{}{}.p'.format(path_to_save,name), 'wb') as fp:\n",
    "            pickle.dump(data, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    from keras.utils import plot_model\n",
    "    dummy_model = get_full_arch(100)\n",
    "\n",
    "    plot_model(dummy_model, to_file='{}model.png'.format(path_to_save))\n",
    "\n",
    "    # Open the file\n",
    "    with open(path_to_save + 'report.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        dummy_model.summary(print_fn=lambda x: fh.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_save = '../FULL_ARCH_RESULTS/tmp/'\n",
    "try:\n",
    "    os.mkdir(path_to_save)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def box_plot_methods(method_test_dict, method_val_dict, samples):\n",
    "    metric = []\n",
    "    methods = []\n",
    "    test_val = []\n",
    "    \n",
    "    METRIC_VAL_LABELS = ['VAL'] * samples\n",
    "    METRIC_TEST_LABELS = ['TEST'] * samples\n",
    "    \n",
    "    method_names = method_test_dict.keys()\n",
    "    for name in method_names:\n",
    "        metric += list(method_val_dict[name])\n",
    "        methods += [name] * samples\n",
    "        test_val += METRIC_VAL_LABELS\n",
    "        \n",
    "        metric += list(method_test_dict[name])\n",
    "        methods += [name] * samples\n",
    "        test_val += METRIC_TEST_LABELS\n",
    "        \n",
    "\n",
    "    \n",
    "    print(len(metric), len(methods), len(test_val))\n",
    "    df = pd.DataFrame({\"METRIC\" : metric , \"Method\" : methods , \"TEST/VAL\" : test_val})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg_dict = {}\n",
    "['ROC-AUC' , 'MSE', 'LOGLOSS' , 'MAE']\n",
    "\n",
    "def iter_dict_and_update(metric_dict):\n",
    "    global test_avg_dict\n",
    "    \n",
    "    for key in metric_dict:\n",
    "        if key not in test_avg_dict:\n",
    "            test_avg_dict[key] = []\n",
    "        test_avg_dict[key] += [np.mean(metric_dict[key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_val_dict(val_keyword, test_keyword):\n",
    "    test_dict = {}\n",
    "    val_dict = {}\n",
    "    for i, name in enumerate(order):\n",
    "        test_dict[name] = order_results[i][test_keyword]\n",
    "        val_dict[name] = order_results[i][val_keyword]\n",
    "    \n",
    "    return val_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_test_val_dict(val_keyword, test_keyword):\n",
    "    test_dict = {}\n",
    "    val_dict = {}\n",
    "    for i, name in enumerate(order):\n",
    "        val_results = order_results[i][val_keyword]\n",
    "        test_results = order_results[i][test_keyword]\n",
    "        \n",
    "        if np.mean(val_results) > np.mean(test_results):\n",
    "            index = val_results.index(max(val_results))\n",
    "        else:\n",
    "            index = val_results.index(min(val_results))\n",
    "        \n",
    "        print(val_results)\n",
    "        test_dict[name] = [test_results[index]]\n",
    "        val_dict[name] = [val_results[index]]\n",
    "    \n",
    "    return val_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict, test_dict = get_test_val_dict(\"auc_vals\", \"auc_tests\")\n",
    "df_res = box_plot_methods(test_dict, val_dict , samp)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"TEST/VAL\", y=\"METRIC\",\n",
    "                hue=\"Method\",\n",
    "                data=df_res, kind=\"box\",\n",
    "                height=7.5, aspect=1, legend=False)\n",
    "\n",
    "g.map(plt.axvline, x=0.5, ls='--', c='black')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5), ncol=1)\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"CARS Dataset\")\n",
    "plt.savefig('{}auc_arc.png'.format(path_to_save), bbox_inches=\"tight\")\n",
    "iter_dict_and_update(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict, test_dict = get_test_val_dict(\"mse_vals\", \"mse_tests\")\n",
    "\n",
    "df = box_plot_methods(test_dict, val_dict , samp)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"TEST/VAL\", y=\"METRIC\",\n",
    "                hue=\"Method\",\n",
    "                data=df, kind=\"box\",\n",
    "                height=7.5, aspect=1, legend=False)\n",
    "g.map(plt.axvline, x=0.5, ls='--', c='black')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5), ncol=1)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"CARS Dataset\")\n",
    "plt.savefig('{}mse_arc.png'.format(path_to_save), bbox_inches=\"tight\")\n",
    "iter_dict_and_update(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict, test_dict = get_test_val_dict(\"logloss_vals\", \"logloss_tests\")\n",
    "\n",
    "df = box_plot_methods(test_dict, val_dict , samp)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"TEST/VAL\", y=\"METRIC\",\n",
    "                hue=\"Method\",\n",
    "                data=df, kind=\"box\",\n",
    "                height=7.5, aspect=1, legend=False)\n",
    "g.map(plt.axvline, x=0.5, ls='--', c='black')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5), ncol=1)\n",
    "plt.ylabel(\"logloss\")\n",
    "plt.title(\"CARS Dataset\")\n",
    "g.savefig(\"{}logloss_arc.png\".format(path_to_save))\n",
    "iter_dict_and_update(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test_dict = {}\n",
    "for i, name in enumerate(order):\n",
    "    test_dict[name] = order_results[i]\n",
    "    \n",
    "for key in test_dict.keys():\n",
    "    c_dict = test_dict[key]\n",
    "    preds = c_dict['y_test_pred']\n",
    "    preds_val = c_dict['y_val_pred']\n",
    "    \n",
    "    maes_test = list(map(lambda y_pred : mean_absolute_error(y_test, y_pred), preds))\n",
    "    c_dict[\"mae_tests\"] = maes_test\n",
    "    \n",
    "    maes_val = list(map(lambda y_pred : mean_absolute_error(y_val, y_pred), preds_val))\n",
    "    c_dict[\"mae_vals\"] = maes_val\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.mean(maes_test)\n",
    "    test_avg_dict[key] += [preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict, test_dict = get_test_val_dict(\"mae_vals\", \"mae_tests\")\n",
    "\n",
    "df = box_plot_methods(test_dict, val_dict , samp)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"TEST/VAL\", y=\"METRIC\",\n",
    "                hue=\"Method\",\n",
    "                data=df, kind=\"box\",\n",
    "                height=7.5, aspect=1, legend=False)\n",
    "g.map(plt.axvline, x=0.5, ls='--', c='black')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5), ncol=1)\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"CARS Dataset\")\n",
    "g.savefig(\"{}mae_arc.png\".format(path_to_save))\n",
    "iter_dict_and_update(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['AUC-ROC', 'MSE', 'LOGLOSS', 'MAE','?']\n",
    "indexes = []\n",
    "averages = []\n",
    "\n",
    "for key in test_avg_dict.keys():\n",
    "    indexes += [key]\n",
    "    averages += [tuple(test_avg_dict[key])]\n",
    "    \n",
    "df_averages = pd.DataFrame(averages, columns = columns, index=indexes)\n",
    "df_averages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eliad)",
   "language": "python",
   "name": "eliad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
